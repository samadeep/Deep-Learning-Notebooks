{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Function Implementations with Numpy : Neural Networks as Basic Building Blocks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Building basic functions with numpy\n",
    "\n",
    "### 1.1 - Sigmoid function, np.exp()\n",
    "\n",
    "Build a function that returns the sigmoid of a real number x. Use math.exp(x) for the exponential function.\n",
    "\n",
    "**Reminder**:\n",
    "$sigmoid(x) = \\frac{1}{1+e^{-x}}$ is sometimes also known as the logistic function. It is a non-linear function used not only in Machine Learning (Logistic Regression), but also in Deep Learning.\n",
    "\n",
    "<img src=\"images/Sigmoid.png\" style=\"width:500px;height:228px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9525741268224334"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def basic_sigmoid(x):\n",
    "    s = 1 / (1 + math.exp(-x))\n",
    "    return s\n",
    "\n",
    "basic_sigmoid(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Sigmoid function, np.exp() - Using numpy instead of math\n",
    "\n",
    "In fact, if $ x = (x_1, x_2, ..., x_n)$ is a row vector then $np.exp(x)$ will apply the exponential function to every element of x. The output will thus be: $np.exp(x) = (e^{x_1}, e^{x_2}, ..., e^{x_n})$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.71828183  7.3890561  20.08553692]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([1, 2, 3])\n",
    "print(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mCall signature:\u001b[0m  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mType:\u001b[0m            ufunc\n",
      "\u001b[0;31mString form:\u001b[0m     <ufunc 'exp'>\n",
      "\u001b[0;31mFile:\u001b[0m            /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/__init__.py\n",
      "\u001b[0;31mDocstring:\u001b[0m      \n",
      "exp(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "\n",
      "Calculate the exponential of all elements in the input array.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "x : array_like\n",
      "    Input values.\n",
      "out : ndarray, None, or tuple of ndarray and None, optional\n",
      "    A location into which the result is stored. If provided, it must have\n",
      "    a shape that the inputs broadcast to. If not provided or None,\n",
      "    a freshly-allocated array is returned. A tuple (possible only as a\n",
      "    keyword argument) must have length equal to the number of outputs.\n",
      "where : array_like, optional\n",
      "    This condition is broadcast over the input. At locations where the\n",
      "    condition is True, the `out` array will be set to the ufunc result.\n",
      "    Elsewhere, the `out` array will retain its original value.\n",
      "    Note that if an uninitialized `out` array is created via the default\n",
      "    ``out=None``, locations within it where the condition is False will\n",
      "    remain uninitialized.\n",
      "**kwargs\n",
      "    For other keyword-only arguments, see the\n",
      "    :ref:`ufunc docs <ufuncs.kwargs>`.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "out : ndarray or scalar\n",
      "    Output array, element-wise exponential of `x`.\n",
      "    This is a scalar if `x` is a scalar.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "expm1 : Calculate ``exp(x) - 1`` for all elements in the array.\n",
      "exp2  : Calculate ``2**x`` for all elements in the array.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The irrational number ``e`` is also known as Euler's number.  It is\n",
      "approximately 2.718281, and is the base of the natural logarithm,\n",
      "``ln`` (this means that, if :math:`x = \\ln y = \\log_e y`,\n",
      "then :math:`e^x = y`. For real input, ``exp(x)`` is always positive.\n",
      "\n",
      "For complex arguments, ``x = a + ib``, we can write\n",
      ":math:`e^x = e^a e^{ib}`.  The first term, :math:`e^a`, is already\n",
      "known (it is the real argument, described above).  The second term,\n",
      ":math:`e^{ib}`, is :math:`\\cos b + i \\sin b`, a function with\n",
      "magnitude 1 and a periodic phase.\n",
      "\n",
      "References\n",
      "----------\n",
      ".. [1] Wikipedia, \"Exponential function\",\n",
      "       https://en.wikipedia.org/wiki/Exponential_function\n",
      ".. [2] M. Abramovitz and I. A. Stegun, \"Handbook of Mathematical Functions\n",
      "       with Formulas, Graphs, and Mathematical Tables,\" Dover, 1964, p. 69,\n",
      "       https://personal.math.ubc.ca/~cbm/aands/page_69.htm\n",
      "\n",
      "Examples\n",
      "--------\n",
      "Plot the magnitude and phase of ``exp(x)`` in the complex plane:\n",
      "\n",
      ">>> import matplotlib.pyplot as plt\n",
      "\n",
      ">>> x = np.linspace(-2*np.pi, 2*np.pi, 100)\n",
      ">>> xx = x + 1j * x[:, np.newaxis] # a + ib over complex plane\n",
      ">>> out = np.exp(xx)\n",
      "\n",
      ">>> plt.subplot(121)\n",
      ">>> plt.imshow(np.abs(out),\n",
      "...            extent=[-2*np.pi, 2*np.pi, -2*np.pi, 2*np.pi], cmap='gray')\n",
      ">>> plt.title('Magnitude of exp(x)')\n",
      "\n",
      ">>> plt.subplot(122)\n",
      ">>> plt.imshow(np.angle(out),\n",
      "...            extent=[-2*np.pi, 2*np.pi, -2*np.pi, 2*np.pi], cmap='hsv')\n",
      ">>> plt.title('Phase (angle) of exp(x)')\n",
      ">>> plt.show()\n",
      "\u001b[0;31mClass docstring:\u001b[0m\n",
      "Functions that operate element by element on whole arrays.\n",
      "\n",
      "To see the documentation for a specific ufunc, use `info`.  For\n",
      "example, ``np.info(np.sin)``.  Because ufuncs are written in C\n",
      "(for speed) and linked into Python with NumPy's ufunc facility,\n",
      "Python's help() function finds this page whenever help() is called\n",
      "on a ufunc.\n",
      "\n",
      "A detailed explanation of ufuncs can be found in the docs for :ref:`ufuncs`.\n",
      "\n",
      "**Calling ufuncs:** ``op(*x[, out], where=True, **kwargs)``\n",
      "\n",
      "Apply `op` to the arguments `*x` elementwise, broadcasting the arguments.\n",
      "\n",
      "The broadcasting rules are:\n",
      "\n",
      "* Dimensions of length 1 may be prepended to either array.\n",
      "* Arrays may be repeated along dimensions of length 1.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "*x : array_like\n",
      "    Input arrays.\n",
      "out : ndarray, None, or tuple of ndarray and None, optional\n",
      "    Alternate array object(s) in which to put the result; if provided, it\n",
      "    must have a shape that the inputs broadcast to. A tuple of arrays\n",
      "    (possible only as a keyword argument) must have length equal to the\n",
      "    number of outputs; use None for uninitialized outputs to be\n",
      "    allocated by the ufunc.\n",
      "where : array_like, optional\n",
      "    This condition is broadcast over the input. At locations where the\n",
      "    condition is True, the `out` array will be set to the ufunc result.\n",
      "    Elsewhere, the `out` array will retain its original value.\n",
      "    Note that if an uninitialized `out` array is created via the default\n",
      "    ``out=None``, locations within it where the condition is False will\n",
      "    remain uninitialized.\n",
      "**kwargs\n",
      "    For other keyword-only arguments, see the :ref:`ufunc docs <ufuncs.kwargs>`.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "r : ndarray or tuple of ndarray\n",
      "    `r` will have the shape that the arrays in `x` broadcast to; if `out` is\n",
      "    provided, it will be returned. If not, `r` will be allocated and\n",
      "    may contain uninitialized values. If the function has more than one\n",
      "    output, then the result will be a tuple of arrays."
     ]
    }
   ],
   "source": [
    "??np.exp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the sigmoid function using numpy.\n",
    "\n",
    "**Instructions**: x could now be either a real number, a vector, or a matrix. The data structures we use in numpy to represent these shapes (vectors, matrices...) are called numpy arrays. You don't need to know more for now.\n",
    "\n",
    "$$\n",
    "\\text{For } x \\in \\mathbb{R}^n \\text{,     } sigmoid(x) = sigmoid\\begin{pmatrix}\n",
    "    x_1  \\\\\n",
    "    x_2  \\\\\n",
    "    ...  \\\\\n",
    "    x_n  \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    \\frac{1}{1+e^{-x_1}}  \\\\\n",
    "    \\frac{1}{1+e^{-x_2}}  \\\\\n",
    "    ...  \\\\\n",
    "    \\frac{1}{1+e^{-x_n}}  \\\\\n",
    "\\end{pmatrix}\\tag{1}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    s = 1 / (np.exp(-x) + 1)\n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "sig_x = sigmoid(x)\n",
    "\n",
    "isinstance(sig_x, np.ndarray)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Sigmoid gradient\n",
    "\n",
    "Implement the function sigmoid_grad() to compute the gradient of the sigmoid function with respect to its input x. The formula is: $$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}$$\n",
    "You often code this function in two steps:\n",
    "\n",
    "1. Set s to be the sigmoid of x. You might find your sigmoid(x) function useful.\n",
    "2. Compute $\\sigma'(x) = s(1-s)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid_derivative(x) = [0.19661193 0.10499359 0.04517666]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    ds = s * (1 - s)\n",
    "    return ds\n",
    "\n",
    "x = np.array([1, 2, 3])\n",
    "print(\"sigmoid_derivative(x) = \" + str(sigmoid_derivative(x)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Reshaping arrays : An insight into Image reading and Conversion into Numpy arrays\n",
    "\n",
    "Two common numpy functions used in deep learning are [np.shape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html) and [np.reshape()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html).\n",
    "\n",
    "- X.shape is used to get the shape (dimension) of a matrix/vector X.\n",
    "- X.reshape(...) is used to reshape X into some other dimension.\n",
    "\n",
    "For example, in computer science, an image is represented by a 3D array of shape $(length, height, depth = 3)$. However, when you read an image as the input of an algorithm you convert it to a vector of shape $(length*height*3, 1)$. In other words, you \"unroll\", or reshape, the 3D array into a 1D vector.\n",
    "\n",
    "<img src=\"images/image2vector_kiank.png\" style=\"width:500px;height:300;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img2vector(image):\n",
    "    \n",
    "    ''' Argument:\n",
    "\n",
    "    image -- a numpy array of shape (length, height, depth)\n",
    "    \n",
    "    Returns:\n",
    "    v -- a vector of shape (length*height*depth, 1)\n",
    "    default depth = 3 for R G B values \n",
    "\n",
    "    '''\n",
    "    v = image.reshape(image.shape[0] * image.shape[1] * image.shape[2] , 1)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450, 800, 3)\n",
      "image2vector(image) = [[25]\n",
      " [26]\n",
      " [12]\n",
      " ...\n",
      " [66]\n",
      " [85]\n",
      " [81]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m7/zxjxt1x96b78g7vf25hyc5f00000gn/T/ipykernel_6956/629737279.py:4: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  img = imageio.imread('images/img].jpeg')\n"
     ]
    }
   ],
   "source": [
    "# Read an Image and convert it to a vector\n",
    "import numpy as np\n",
    "import imageio\n",
    "img = imageio.imread('images/img].jpeg')\n",
    "print(img.shape)\n",
    "\n",
    "# https://www.pluralsight.com/guides/importing-image-data-into-numpy-arrays\n",
    "\n",
    "image = np.asarray(img)\n",
    "print (\"image2vector(image) = \" + str(img2vector(image)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - Normalizing rows\n",
    "\n",
    "Another common technique we use in Machine Learning and Deep Learning is to normalize our data. It often leads to a better performance because gradient descent converges faster after normalization. Here, by normalization we mean changing x to $ \\frac{x}{\\| x\\|} $ (dividing each row vector of x by its norm).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if $$x = \n",
    "\\begin{bmatrix}\n",
    "    0 & 3 & 4 \\\\\n",
    "    2 & 6 & 4 \\\\\n",
    "\\end{bmatrix}\\tag{3}$$ then $$\\| x\\| = np.linalg.norm(x, axis = 1, keepdims = True) = \\begin{bmatrix}\n",
    "    5 \\\\\n",
    "    \\sqrt{56} \\\\\n",
    "\\end{bmatrix}\\tag{4} $$and        $$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
    "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
    "\\end{bmatrix}\\tag{5}$$\n",
    "Implement normalizeRows() to normalize the rows of a matrix. After applying this function to an input matrix x, each row of x should be a vector of unit length (meaning length 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalizeRows(x) = [[0.         0.6        0.8       ]\n",
      " [0.13736056 0.82416338 0.54944226]]\n"
     ]
    }
   ],
   "source": [
    "def normalizeRows(x):\n",
    "\n",
    "    x_norm = np.linalg.norm(x , axis = 1 , keepdims = True)\n",
    "    # Divide x by its norm.\n",
    "    x = x/x_norm\n",
    "    return x\n",
    "\n",
    "x = np.array([\n",
    "    [0, 3, 4],\n",
    "    [1, 6, 4]])\n",
    "print(\"normalizeRows(x) = \" + str(normalizeRows(x)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 - Broadcasting and the softmax function\n",
    "\n",
    "A very important concept to understand in numpy is \"broadcasting\". It is very useful for performing mathematical operations between arrays of different shapes. For the full details on broadcasting, you can read the official [broadcasting documentation](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).\n",
    "\n",
    "#### Implement a softmax function using numpy. You can think of softmax as a normalizing function used when your algorithm needs to classify two or more classes. You will learn more about softmax in the second course of this specialization.\n",
    "\n",
    "- $ \\text{for } x \\in \\mathbb{R}^{1\\times n} \\text{, } softmax(x) = softmax(\\begin{bmatrix}\n",
    "  x*1 &&\n",
    "  x_2 &&\n",
    "  ... &&\n",
    "  x_n  \n",
    "  \\end{bmatrix}) = \\begin{bmatrix}\n",
    "  \\frac{e^{x_1}}{\\sum*{j}e^{x*j}} &&\n",
    "  \\frac{e^{x_2}}{\\sum*{j}e^{x*j}} &&\n",
    "  ... &&\n",
    "  \\frac{e^{x_n}}{\\sum*{j}e^{x_j}}\n",
    "  \\end{bmatrix} $\n",
    "\n",
    "- $\\text{for a matrix } x \\in \\mathbb{R}^{m \\times n} \\text{,  $x_{ij}$ maps to the element in the $i^{th}$ row and $j^{th}$ column of $x$, thus we have: }$ $$softmax(x) = softmax\\begin{bmatrix}\n",
    "    x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
    "    x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
    "\\end{bmatrix} = \\begin{pmatrix}\n",
    "    softmax\\text{(first row of x)}  \\\\\n",
    "    softmax\\text{(second row of x)} \\\\\n",
    "    ...  \\\\\n",
    "    softmax\\text{(last row of x)} \\\\\n",
    "\\end{pmatrix} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x_exp = np.exp(x)\n",
    "    x_sum = np.sum(x_exp , axis = 1 , keepdims = True)\n",
    "    s = x_exp/x_sum\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.        ,  20.08553692,  54.59815003],\n",
       "       [  2.71828183, 403.42879349,  54.59815003]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_exp = np.exp(x)\n",
    "x_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 75.68368696],\n",
       "       [460.74522535]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sum = np.sum(x_exp , axis = 1 , keepdims = True)\n",
    "x_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01321289, 0.26538793, 0.72139918],\n",
       "       [0.00589975, 0.8756006 , 0.11849965]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = x_exp/x_sum\n",
    "s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Vectorization\n",
    "\n",
    "In deep learning, you deal with very large datasets.\n",
    "\n",
    "Hence, a non-computationally-optimal function can become a huge bottleneck in your algorithm and can result in a model that takes ages to run. To make sure that your code is computationally efficient, you will use vectorization. For example, try to tell the difference between the following implementations of the dot/outer/elementwise product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot = 78\n",
      " ----- Computation time = 0.053999999999998494ms\n",
      "outer = [[ 0. 18. 45.  0.  0. 63.  0.  0.  0.]\n",
      " [ 0.  4. 10.  0.  0. 14.  0.  0.  0.]\n",
      " [ 0. 10. 25.  0.  0. 35.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. 14. 35.  0.  0. 49.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      " ----- Computation time = 0.0969999999997917ms\n",
      "gdot = [12.94580715  8.95419293  9.18348223]\n",
      " ----- Computation time = 0.12100000000003774ms\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "x1 = [9, 2, 5, 0, 0, 7, 0, 0, 0]\n",
    "x2 = [0, 2, 5, 0, 0, 7, 0, 0, 0]\n",
    "\n",
    "\n",
    "### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###\\\n",
    "tic = time.process_time()\n",
    "dot = 0\n",
    "for i in range(len(x1)):\n",
    "    dot+= x1[i]*x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "\n",
    "\n",
    "### CLASSIC OUTER PRODUCT IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "outer = np.zeros((len(x1),len(x2))) # we create a len(x1)*len(x2) matrix with only zeros\n",
    "for i in range(len(x1)):\n",
    "    for j in range(len(x2)):\n",
    "        outer[i,j] = x1[i]*x2[j]\n",
    "toc = time.process_time()\n",
    "print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "\n",
    "### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###\n",
    "W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array\n",
    "tic = time.process_time()\n",
    "gdot = np.zeros(W.shape[0])\n",
    "\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(len(x1)):\n",
    "        gdot[i] += W[i,j]*x1[j]\n",
    "toc = time.process_time()\n",
    "print (\"gdot = \" + str(gdot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "x1 = [9, 2, 5, 0, 0, 7, 0, 0, 0]\n",
    "W = np.random.rand(3,len(x1))     # Random array with  3*len(x1) numpy array\n",
    "\n",
    "print(len(W[0]))\n",
    "print(len(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot with numpy = 78\n",
      " ----- Computation time = 0.05500000000013827ms\n",
      "outer = [[ 0 18 45  0  0 63  0  0  0]\n",
      " [ 0  4 10  0  0 14  0  0  0]\n",
      " [ 0 10 25  0  0 35  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0 14 35  0  0 49  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]]\n",
      " ----- Computation time = 0.08300000000005525ms\n",
      "gdot = [14.09890273 14.13493541 11.00269118]\n",
      " ----- Computation time = 0.02799999999991698ms\n"
     ]
    }
   ],
   "source": [
    "# Implemntation of Vectorization using Numpy and timing it\n",
    "x1 = [9, 2, 5, 0, 0, 7, 0, 0, 0]\n",
    "x2 = [0, 2, 5, 0, 0, 7, 0, 0, 0]\n",
    "\n",
    "tic = time.process_time()\n",
    "dot = np.dot(x1,x2)\n",
    "toc = time.process_time()\n",
    "\n",
    "print (\"dot with numpy = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "\n",
    "### VECTORIZED OUTER PRODUCT ###\n",
    "tic = time.process_time()\n",
    "outer = np.outer(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED GENERAL DOT PRODUCT ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(W,x1)\n",
    "toc = time.process_time()\n",
    "print (\"gdot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that `np.dot()` performs a matrix-matrix or matrix-vector multiplication. This is different from `np.multiply()` and the `*` operator (which is equivalent to `.*` in Matlab/Octave), which performs an element-wise multiplication.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Implement the L1 and L2 loss functions\n",
    "\n",
    "L1 Loss Function :\n",
    "\n",
    "Implement the numpy vectorized version of the L1 loss. You may find the function abs(x) (absolute value of x) useful.\n",
    "\n",
    "**Reminder**:\n",
    "\n",
    "- The loss is used to evaluate the performance of your model. The bigger your loss is, the more different your predictions ($ \\hat{y} $) are from the true values ($y$). In deep learning, you use optimization algorithms like Gradient Descent to train your model and to minimize the cost.\n",
    "- L1 loss is defined as:\n",
    "  $$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{6}$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 Loss Function :\n",
    "\n",
    "Implement the numpy vectorized version of the L2 loss. There are several way of implementing the L2 loss but you may find the function np.dot() useful. As a reminder, if $x = [x_1, x_2, ..., x_n]$, then `np.dot(x,x)` = $\\sum_{j=0}^n x_j^{2}$.\n",
    "\n",
    "- L2 loss is defined as $$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L1(yhat , y):\n",
    "    loss = np.sum(np.abs(yhat - y))\n",
    "    return loss\n",
    "\n",
    "def L2(yhat , y):\n",
    "    loss = np.sum(np.dot(yhat - y , yhat - y))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 = 1.1\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L1 = \" + str(L1(yhat,y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 = 0.43000000000000005\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L2 = \" + str(L2(yhat,y)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
